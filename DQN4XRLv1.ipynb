{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nabazar/AI-Aimbot/blob/main/DQN4XRLv1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCoNZvfmOA33",
        "outputId": "635bb056-38df-4865-9cda-82425519dd33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'gym-macro-overcooked' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone  https://github.com/WeihaoTan/gym-macro-overcooked.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "try:\n",
        "    import shap\n",
        "except ImportError:\n",
        "  !pip install shap\n",
        "  import  shap"
      ],
      "metadata": {
        "id": "Jf1NJLPjiBgK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#\n",
        "try:\n",
        "    import lime\n",
        "except ImportError:\n",
        "  !pip install lime\n",
        "  import  lime"
      ],
      "metadata": {
        "id": "yvw_Nl_-lJmj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "\n",
        "import pygame\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# from .render.game import Game\n",
        "from gym import spaces\n",
        "# from .items import Tomato, Lettuce, Onion, Plate, Knife, Delivery, Agent, Food\n",
        "import copy"
      ],
      "metadata": {
        "id": "UfS829YoO8J3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ipyBhfMarL6"
      },
      "source": [
        "#Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GRlDjDuq5L4i"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import Dict, List, Tuple\n",
        "import pickle\n",
        "import numpy.matlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "E3biPTV3cl_m"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "import os\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn import preprocessing\n",
        "from gym import spaces\n",
        "from gym.spaces import Tuple , Box\n",
        "\n",
        "\n",
        "import random\n",
        "from collections import deque\n",
        "from typing import Deque, Dict, List, Tuple\n",
        "\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "from torch.distributions import Normal"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O93zVcJ7t-Xo"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hHGw_E4cuD4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title PPO agent\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class PPOAgent:\n",
        "    def __init__(self, env, state_size, action_size, lr=3e-4, gamma=0.99, clip_ratio=0.2, update_epochs=10, batch_size=64):\n",
        "        self.env = env\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "        self.clip_ratio = clip_ratio\n",
        "        self.update_epochs = update_epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.policy = PolicyNet(self.state_size, self.action_size)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=self.lr)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        probs = self.policy(state).squeeze(0)\n",
        "        dist = Categorical(probs)\n",
        "        action = dist.sample().item()\n",
        "        return action\n",
        "\n",
        "    def update(self, states, actions, rewards, dones, next_states):\n",
        "        \"\"\"Update the policy network\"\"\"\n",
        "        states = torch.from_numpy(np.array(states)).float()\n",
        "        actions = torch.from_numpy(np.array(actions)).long()\n",
        "        rewards = torch.from_numpy(np.array(rewards)).float()\n",
        "        dones = torch.from_numpy(np.array(dones)).float()\n",
        "        next_states = torch.from_numpy(np.array(next_states)).float()\n",
        "\n",
        "        # Calculate discounted rewards\n",
        "        returns = self.calculate_returns(rewards, dones)\n",
        "\n",
        "        for _ in range(self.update_epochs):\n",
        "            # Get the current action probabilities\n",
        "            probs = self.policy(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            # Get the old action probabilities\n",
        "            with torch.no_grad():\n",
        "                old_probs = self.policy(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            # Calculate the ratio between new and old probabilities\n",
        "            ratios = probs / old_probs\n",
        "\n",
        "            # Calculate the surrogate loss\n",
        "            advantage = returns - self.policy.value(states).squeeze()\n",
        "            surr1 = ratios * advantage\n",
        "            surr2 = torch.clamp(ratios, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantage\n",
        "            loss = -torch.mean(torch.min(surr1, surr2))\n",
        "\n",
        "            # Update the policy network\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "    def calculate_returns(self, rewards, dones):\n",
        "        \"\"\"Calculate the discounted returns\"\"\"\n",
        "        returns = []\n",
        "        R = 0\n",
        "        for r, d in zip(reversed(rewards), reversed(dones)):\n",
        "            R = r + self.gamma * R * (1 - d)\n",
        "            returns.insert(0, R)\n",
        "        return torch.tensor(returns)\n",
        "\n",
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc_pi = nn.Linear(64, action_size)\n",
        "        self.fc_v = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        pi = torch.softmax(self.fc_pi(x), dim=-1)\n",
        "        v = self.fc_v(x)\n",
        "        return pi, v\n",
        "\n",
        "    def value(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc_v(x)"
      ],
      "metadata": {
        "id": "9huxD97DAnqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Policy Gradient"
      ],
      "metadata": {
        "id": "Fp--O22aTzju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class PolicyGradientAgent:\n",
        "    def __init__(self, env, state_size, action_size, lr=1e-3, gamma=0.99):\n",
        "        self.env = env\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.lr = lr\n",
        "        self.gamma = gamma\n",
        "\n",
        "        self.policy = PolicyNet(self.state_size, self.action_size)\n",
        "        self.optimizer = optim.Adam(self.policy.parameters(), lr=self.lr)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        probs, _ = self.policy(state)\n",
        "        dist = Categorical(probs)\n",
        "        action = dist.sample().item()\n",
        "        return action\n",
        "\n",
        "    def update(self, states, actions, rewards):\n",
        "        \"\"\"Update the policy network\"\"\"\n",
        "        states = torch.from_numpy(np.array(states)).float()\n",
        "        actions = torch.from_numpy(np.array(actions)).long()\n",
        "        rewards = torch.from_numpy(np.array(rewards)).float()\n",
        "\n",
        "        # Calculate the discounted rewards\n",
        "        returns = self.calculate_returns(rewards)\n",
        "\n",
        "        # Compute the policy loss\n",
        "        probs, _ = self.policy(states)\n",
        "        dist = Categorical(probs)\n",
        "        log_probs = dist.log_prob(actions)\n",
        "        policy_loss = -torch.mean(log_probs * returns)\n",
        "\n",
        "        # Update the policy network\n",
        "        self.optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def calculate_returns(self, rewards):\n",
        "        \"\"\"Calculate the discounted returns\"\"\"\n",
        "        returns = []\n",
        "        R = 0\n",
        "        for r in reversed(rewards):\n",
        "            R = r + self.gamma * R\n",
        "            returns.insert(0, R)\n",
        "        return torch.tensor(returns)\n",
        "\n",
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc_pi = nn.Linear(64, action_size)\n",
        "        self.fc_v = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        pi = torch.softmax(self.fc_pi(x), dim=-1)\n",
        "        v = self.fc_v(x)\n",
        "        return pi, v\n",
        "\n",
        "    def value(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        return self.fc_v(x)"
      ],
      "metadata": {
        "id": "NP_UWFlAA87e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "class PolicyGradientAgent:\n",
        "    def __init__(self, env, lr=0.01, gamma=0.99):\n",
        "        self.env = env\n",
        "        self.state_size = env.obs_space.shape[0]\n",
        "        self.action_size = env.action_space.n\n",
        "\n",
        "        self.policy_net = nn.Sequential(\n",
        "            nn.Linear(self.state_size, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, self.action_size),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def select_action(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
        "        probs = self.policy_net(state)\n",
        "        dist = Categorical(probs)\n",
        "        action = dist.sample()\n",
        "        return action.item()\n",
        "\n",
        "    def update_policy(self, rewards, states, actions):\n",
        "        discounted_rewards = self.discount_rewards(rewards)\n",
        "        states = torch.from_numpy(np.array(states)).float()\n",
        "        actions = torch.tensor(actions)\n",
        "\n",
        "        probs = self.policy_net(states)\n",
        "        dist = Categorical(probs)\n",
        "        log_probs = dist.log_prob(actions)\n",
        "\n",
        "        loss = -torch.mean(log_probs * discounted_rewards)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "    def discount_rewards(self, rewards):\n",
        "        discounted_rewards = np.zeros_like(rewards)\n",
        "        running_add = 0\n",
        "        for i in reversed(range(len(rewards))):\n",
        "            running_add = running_add * self.gamma + rewards[i]\n",
        "            discounted_rewards[i] = running_add\n",
        "        return (discounted_rewards - np.mean(discounted_rewards)) / (np.std(discounted_rewards) + 1e-8)\n"
      ],
      "metadata": {
        "id": "vas7G-QPkHi4"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "c1ImJhPR5L4l"
      },
      "outputs": [],
      "source": [
        "# @title Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
        "\n",
        "    def __init__(self, obs_dim: int, size: int, batch_size: int = 32):\n",
        "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size, = 0, 0\n",
        "\n",
        "    def store(\n",
        "        self,\n",
        "        obs: np.ndarray,\n",
        "        act: np.ndarray,\n",
        "        rew: float,\n",
        "        next_obs: np.ndarray,\n",
        "        done: bool,\n",
        "    ):\n",
        "\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
        "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
        "        return dict(obs=self.obs_buf[idxs],\n",
        "                    next_obs=self.next_obs_buf[idxs],\n",
        "                    acts=self.acts_buf[idxs],\n",
        "                    rews=self.rews_buf[idxs],\n",
        "                    done=self.done_buf[idxs])\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return self.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "uoYXwtyi5L4o"
      },
      "outputs": [],
      "source": [
        "# @title Network\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, in_dim: int, out_dim: int):\n",
        "        \"\"\"Initialization.\"\"\"\n",
        "        super(Network, self).__init__()\n",
        "        print('out_dim',out_dim)\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(in_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "X8cZduDd5L4r"
      },
      "outputs": [],
      "source": [
        "# @title DQN agent\n",
        "class DQNAgent:\n",
        "    \"\"\"DQN Agent interacting with environment.\n",
        "\n",
        "    Attribute:\n",
        "        env (gym.Env): openAI Gym environment\n",
        "        memory (ReplayBuffer): replay memory to store transitions\n",
        "        batch_size (int): batch size for sampling\n",
        "        epsilon (float): parameter for epsilon greedy policy\n",
        "        epsilon_decay (float): step size to decrease epsilon\n",
        "        max_epsilon (float): max value of epsilon\n",
        "        min_epsilon (float): min value of epsilon\n",
        "        target_update (int): period for target model's hard update\n",
        "        gamma (float): discount factor\n",
        "        dqn (Network): model to train and select actions\n",
        "        dqn_target (Network): target model to update\n",
        "        optimizer (torch.optim): optimizer for training dqn\n",
        "        transition (list): transition information including\n",
        "                           state, action, reward, next_state, done\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        memory_size: int,\n",
        "        batch_size: int,\n",
        "        target_update: int,\n",
        "        epsilon_decay: float,\n",
        "        max_epsilon: float = 1.0,\n",
        "        min_epsilon: float = 0.1,\n",
        "        gamma: float = 0.99,\n",
        "    ):\n",
        "        \"\"\"Initialization.\n",
        "\n",
        "        Args:\n",
        "            env (gym.Env): openAI Gym environment\n",
        "            memory_size (int): length of memory\n",
        "            batch_size (int): batch size for sampling\n",
        "            target_update (int): period for target model's hard update\n",
        "            epsilon_decay (float): step size to decrease epsilon\n",
        "            lr (float): learning rate\n",
        "            max_epsilon (float): max value of epsilon\n",
        "            min_epsilon (float): min value of epsilon\n",
        "            gamma (float): discount factor\n",
        "        \"\"\"\n",
        "        obs_dim = env.obs_space.shape[0]\n",
        "        action_dim = env.action_space.n\n",
        "\n",
        "        self.env = env\n",
        "        self.memory = ReplayBuffer(obs_dim, memory_size, batch_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.epsilon = max_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "        self.max_epsilon = max_epsilon\n",
        "        self.min_epsilon = min_epsilon\n",
        "        self.target_update = target_update\n",
        "        self.gamma = gamma\n",
        "\n",
        "        # device: cpu / gpu\n",
        "        self.device = torch.device(\n",
        "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        )\n",
        "        print(self.device)\n",
        "\n",
        "        # networks: dqn, dqn_target\n",
        "        self.dqn = Network(obs_dim, action_dim).to(self.device)\n",
        "        self.dqn_target = Network(obs_dim, action_dim).to(self.device)\n",
        "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
        "        self.dqn_target.eval()\n",
        "\n",
        "        # optimizer\n",
        "        self.optimizer = optim.Adam(self.dqn.parameters())\n",
        "\n",
        "        # transition to store in memory\n",
        "        self.transition = list()\n",
        "\n",
        "        # mode: train / test\n",
        "        self.is_test = False\n",
        "\n",
        "    def reset(self):\n",
        "      state=self.env.reset()\n",
        "      state=np.concatenate(state,axis=None)\n",
        "      return state\n",
        "\n",
        "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Select an action from the input state.\"\"\"\n",
        "        # epsilon greedy policy\n",
        "        if  self.is_test:\n",
        "          selected_action = self.dqn(torch.FloatTensor(state).to(self.device)).argmax()\n",
        "          selected_action = selected_action.detach().cpu().numpy()\n",
        "        else:\n",
        "          # self.epsilon=0.3\n",
        "          if self.epsilon > np.random.random():\n",
        "              selected_action = self.env.action_space.sample()\n",
        "          else:\n",
        "              selected_action = self.dqn(\n",
        "                  torch.FloatTensor(state).to(self.device)\n",
        "              ).argmax()\n",
        "              selected_action = selected_action.detach().cpu().numpy()\n",
        "\n",
        "        if not self.is_test:\n",
        "            self.transition = [state, selected_action]\n",
        "\n",
        "        return selected_action\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
        "        next_state, reward,done,info = self.env.step(action)\n",
        "        next_state=np.concatenate(next_state,axis=None)\n",
        "        reward=np.sum(reward)\n",
        "\n",
        "\n",
        "        if not self.is_test:\n",
        "            self.transition += [reward, next_state, done]\n",
        "            self.memory.store(*self.transition)\n",
        "\n",
        "        return next_state, reward, done,info\n",
        "\n",
        "    def update_model(self) -> torch.Tensor:\n",
        "        \"\"\"Update the model by gradient descent.\"\"\"\n",
        "        samples = self.memory.sample_batch()\n",
        "\n",
        "        loss = self._compute_dqn_loss(samples)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def _compute_dqn_loss(self, samples: Dict[str, np.ndarray]) -> torch.Tensor:\n",
        "        \"\"\"Return dqn loss.\"\"\"\n",
        "        device = self.device  # for shortening the following lines\n",
        "        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
        "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
        "        action = torch.LongTensor(samples[\"acts\"].reshape(-1, 1)).to(device)\n",
        "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
        "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
        "\n",
        "        # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n",
        "        #       = r                       otherwise\n",
        "        curr_q_value = self.dqn(state).gather(1, action)\n",
        "        next_q_value = self.dqn_target(\n",
        "            next_state\n",
        "        ).max(dim=1, keepdim=True)[0].detach()\n",
        "        mask = 1 - done\n",
        "        target = (reward + self.gamma * next_q_value * mask).to(self.device)\n",
        "\n",
        "        # calculate dqn loss\n",
        "        loss = F.smooth_l1_loss(curr_q_value, target)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _target_hard_update(self):\n",
        "        \"\"\"Hard update: target <- local.\"\"\"\n",
        "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Color:\n",
        "    BLACK = (0, 0, 0)\n",
        "    FLOOR = (245, 230, 210)  # light gray\n",
        "    COUNTER = (220, 170, 110)   # tan/gray\n",
        "    COUNTER_BORDER = (114, 93, 51)  # darker tan\n",
        "    DELIVERY = (96, 96, 96)  # grey\n",
        "\n",
        "KeyToTuple = {\n",
        "    pygame.K_UP    : ( 0, -1),  #273\n",
        "    pygame.K_DOWN  : ( 0,  1),  #274\n",
        "    pygame.K_RIGHT : ( 1,  0),  #275\n",
        "    pygame.K_LEFT  : (-1,  0),  #276\n",
        "}"
      ],
      "metadata": {
        "id": "brP6ZI9HacES"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Defining Items\n",
        "#!/usr/bin/python\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Item(object):\n",
        "    def __init__(self, pos_x, pos_y):\n",
        "        self.x = pos_x\n",
        "        self.y = pos_y\n",
        "\n",
        "class MovableItem(Item):\n",
        "    def __init__(self, pos_x, pos_y):\n",
        "        super().__init__(pos_x, pos_y)\n",
        "        self.initial_x = pos_x\n",
        "        self.initial_y = pos_y\n",
        "\n",
        "    def move(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def refresh(self):\n",
        "        self.x = self.initial_x\n",
        "        self.y = self.initial_y\n",
        "\n",
        "class Food(MovableItem):\n",
        "    # 0 for unchoopped 1 for chopped\n",
        "    def __init__(self, pos_x, pos_y, chopped = False):\n",
        "        super().__init__(pos_x, pos_y)\n",
        "        self.chopped = chopped\n",
        "        self.cur_chopped_times = 0\n",
        "        self.required_chopped_times = 3\n",
        "\n",
        "    def chop(self):\n",
        "        if not self.chopped:\n",
        "            self.cur_chopped_times += 1\n",
        "            if self.cur_chopped_times >= self.required_chopped_times:\n",
        "                self.chopped = True\n",
        "\n",
        "    def refresh(self):\n",
        "        self.x = self.initial_x\n",
        "        self.y = self.initial_y\n",
        "        self.chopped = False\n",
        "        self.cur_chopped_times = 0\n",
        "\n",
        "class Tomato(Food):\n",
        "    def __init__(self, pos_x, pos_y):\n",
        "        super().__init__(pos_x, pos_y)\n",
        "        self.rawName = \"tomato\"\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        if self.chopped:\n",
        "            return \"ChoppedTomato\"\n",
        "        else:\n",
        "            return \"FreshTomato\"\n",
        "\n",
        "class Lettuce(Food):\n",
        "    def __init__(self, pos_x, pos_y):\n",
        "        super().__init__(pos_x, pos_y)\n",
        "        self.rawName = \"lettuce\"\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        if self.chopped:\n",
        "            return \"ChoppedLettuce\"\n",
        "        else:\n",
        "            return \"FreshLettuce\"\n",
        "\n",
        "class Onion(Food):\n",
        "    def __init__(self, pos_x, pos_y):\n",
        "        super().__init__(pos_x, pos_y)\n",
        "        self.rawName = \"onion\"\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        if self.chopped:\n",
        "            return \"ChoppedOnion\"\n",
        "        else:\n",
        "            return \"FreshOnion\"\n",
        "\n",
        "class FixedItem(Item):\n",
        "    def __init__(self, pos_x, pos_y, holding = None):\n",
        "        super().__init__(pos_x, pos_y)\n",
        "        self.holding = holding\n",
        "\n",
        "    def hold(self, items):\n",
        "        self.holding = items\n",
        "\n",
        "    def release(self):\n",
        "        self.holding = None\n",
        "\n",
        "class Knife(FixedItem):\n",
        "    def __init__(self, pos_x, pos_y, holding = None):\n",
        "        super().__init__(pos_x, pos_y, holding)\n",
        "        self.rawName = \"knife\"\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return \"cutboard\"\n",
        "\n",
        "class Delivery(FixedItem):\n",
        "    def __init__(self, pos_x, pos_y, holding = None):\n",
        "        super().__init__(pos_x, pos_y, holding)\n",
        "        self.rawName = \"delivery\"\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return \"delivery\"\n",
        "\n",
        "\n",
        "class Plate(MovableItem):\n",
        "    def __init__(self, pos_x, pos_y, containing = None):\n",
        "        super().__init__(pos_x, pos_y)\n",
        "        self.containing = containing\n",
        "        self.rawName = \"plate\"\n",
        "\n",
        "    def contain(self, items):\n",
        "        if self.containing:\n",
        "            self.containing.append(items)\n",
        "        else:\n",
        "            self.containing = [items]\n",
        "        for item in self.containing:\n",
        "            item.move(self.x, self.y)\n",
        "\n",
        "    def move(self, x, y):\n",
        "        super().move(x, y)\n",
        "        if self.containing:\n",
        "            for item in self.containing:\n",
        "                item.move(x, y)\n",
        "\n",
        "    def release(self):\n",
        "        self.containing = None\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        return \"plate\"\n",
        "\n",
        "    @property\n",
        "    def containedName(self):\n",
        "        dishName = \"\"\n",
        "        foodList = [Lettuce, Onion, Tomato]\n",
        "        foodInPlate = [-1] * len(foodList)\n",
        "\n",
        "        for f in range(len(self.containing)):\n",
        "            for i in range(len(foodList)):\n",
        "                if isinstance(self.containing[f], foodList[i]):\n",
        "                    foodInPlate[i] = f\n",
        "        for i in range(len(foodList)):\n",
        "            if foodInPlate[i] > -1:\n",
        "                dishName += self.containing[foodInPlate[i]].name + \"-\"\n",
        "        return dishName[:-1]\n",
        "\n",
        "\n",
        "class Agent(MovableItem):\n",
        "    def __init__(self, pos_x, pos_y, holding = None, color = None):\n",
        "        super().__init__(pos_x, pos_y)\n",
        "        self.holding = holding\n",
        "        self.color = color\n",
        "        self.moved = False\n",
        "        self.obs = None\n",
        "        self.pomap = None\n",
        "        self.rawName = \"agent\"\n",
        "\n",
        "    def pickup(self,item):\n",
        "        self.holding = item\n",
        "        item.move(self.x, self.y)\n",
        "\n",
        "    def putdown(self, x, y):\n",
        "        self.holding.move(x, y)\n",
        "        self.holding = None\n",
        "\n",
        "    def move(self, x, y):\n",
        "        super().move(x, y)\n",
        "        self.moved = True\n",
        "        if self.holding:\n",
        "            self.holding.move(x, y)"
      ],
      "metadata": {
        "id": "DwNnSb2DZ9WN",
        "cellView": "form"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Game\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import pygame\n",
        "import numpy as np\n",
        "# from .utils import *\n",
        "# from ..items import Tomato, Lettuce, Plate, Knife, Delivery, Agent, Food\n",
        "\n",
        "# graphics_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), 'graphics'))\n",
        "graphics_dir =\"/content/gym-macro-overcooked/gym_macro_overcooked/render/graphics\"\n",
        "_image_library = {}\n",
        "\n",
        "ITEMNAME = [\"space\", \"counter\", \"agent\", \"tomato\", \"lettuce\", \"plate\", \"knife\", \"delivery\", \"onion\"]\n",
        "ITEMIDX= {\"space\": 0, \"counter\": 1, \"agent\": 2, \"tomato\": 3, \"lettuce\": 4, \"plate\": 5, \"knife\": 6, \"delivery\": 7, \"onion\": 8}\n",
        "\n",
        "def get_image(path):\n",
        "    global _image_library\n",
        "    image = _image_library.get(path)\n",
        "    if image == None:\n",
        "        canonicalized_path = path.replace('/', os.sep).replace('\\\\', os.sep)\n",
        "        image = pygame.image.load(canonicalized_path)\n",
        "        _image_library[path] = image\n",
        "    return image\n",
        "\n",
        "\n",
        "class Game:\n",
        "    def __init__(self, env):\n",
        "        self._running = True\n",
        "        self.env = env\n",
        "\n",
        "        # Visual parameters\n",
        "        self.scale = 80   # num pixels per tile\n",
        "        self.holding_scale = 0.5\n",
        "        self.container_scale = 0.7\n",
        "        self.width = self.scale * self.env.xlen\n",
        "        self.height = self.scale * self.env.ylen\n",
        "        self.tile_size = (self.scale, self.scale)\n",
        "        self.holding_size = tuple((self.holding_scale * np.asarray(self.tile_size)).astype(int))\n",
        "        self.container_size = tuple((self.container_scale * np.asarray(self.tile_size)).astype(int))\n",
        "        self.holding_container_size = tuple((self.container_scale * np.asarray(self.holding_size)).astype(int))\n",
        "\n",
        "        pygame.init()\n",
        "\n",
        "    def on_init(self):\n",
        "        pygame.init()\n",
        "        if self.play:\n",
        "            self.screen = pygame.display.set_mode((self.width, self.height))\n",
        "        else:\n",
        "            # Create a hidden surface\n",
        "            self.screen = pygame.Surface((self.width, self.height))\n",
        "        self._running = True\n",
        "\n",
        "\n",
        "    def on_event(self, event):\n",
        "        if event.type == pygame.QUIT:\n",
        "            self._running = False\n",
        "\n",
        "    def on_render(self):\n",
        "        self.screen = pygame.display.set_mode((self.width, self.height))\n",
        "        self.screen.fill(Color.FLOOR)\n",
        "        for x in range(self.env.xlen):\n",
        "            for y in range(self.env.ylen):\n",
        "                sl = self.scaled_location((y, x))\n",
        "                if self.env.map[x][y] == ITEMIDX[\"counter\"]:\n",
        "                    fill = pygame.Rect(sl[0], sl[1], self.scale, self.scale)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER, fill)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER_BORDER, fill, 1)\n",
        "                elif self.env.map[x][y] == ITEMIDX[\"delivery\"]:\n",
        "                    fill = pygame.Rect(sl[0], sl[1], self.scale, self.scale)\n",
        "                    pygame.draw.rect(self.screen, Color.DELIVERY, fill)\n",
        "                    self.draw('delivery', self.tile_size, sl)\n",
        "                    for k in self.env.delivery:\n",
        "                        if k.x == x and k.y == y:\n",
        "                            if k.holding:\n",
        "                                self.draw(k.holding.name, self.tile_size, sl)\n",
        "                                if k.holding.name == \"plate\":\n",
        "                                    if k.holding.containing:\n",
        "                                        self.draw(k.holding.containedName, self.container_size, self.container_location((y, x)))\n",
        "                elif self.env.map[x][y] == ITEMIDX[\"knife\"]:\n",
        "                    fill = pygame.Rect(sl[0], sl[1], self.scale, self.scale)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER, fill)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER_BORDER, fill, 1)\n",
        "                    self.draw('cutboard', self.tile_size, sl)\n",
        "                    for k in self.env.knife:\n",
        "                        if k.x == x and k.y == y:\n",
        "                            if k.holding:\n",
        "                                self.draw(k.holding.name, self.tile_size, sl)\n",
        "                                if k.holding.name == \"plate\":\n",
        "                                    if k.holding.containing:\n",
        "                                        self.draw(k.holding.containedName, self.container_size, self.container_location((y, x)))\n",
        "                elif self.env.map[x][y] == ITEMIDX[\"tomato\"]:\n",
        "                    fill = pygame.Rect(sl[0], sl[1], self.scale, self.scale)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER, fill)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER_BORDER, fill, 1)\n",
        "                    for t in self.env.tomato:\n",
        "                        if t.x == x and t.y == y:\n",
        "                            self.draw(t.name, self.tile_size, sl)\n",
        "                elif self.env.map[x][y] == ITEMIDX[\"lettuce\"]:\n",
        "                    fill = pygame.Rect(sl[0], sl[1], self.scale, self.scale)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER, fill)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER_BORDER, fill, 1)\n",
        "                    for l in self.env.lettuce:\n",
        "                        if l.x == x and l.y == y:\n",
        "                            self.draw(l.name, self.tile_size, sl)\n",
        "                elif self.env.map[x][y] == ITEMIDX[\"onion\"]:\n",
        "                    fill = pygame.Rect(sl[0], sl[1], self.scale, self.scale)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER, fill)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER_BORDER, fill, 1)\n",
        "                    for l in self.env.onion:\n",
        "                        if l.x == x and l.y == y:\n",
        "                            self.draw(l.name, self.tile_size, sl)\n",
        "                elif self.env.map[x][y] == ITEMIDX[\"plate\"]:\n",
        "                    fill = pygame.Rect(sl[0], sl[1], self.scale, self.scale)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER, fill)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER_BORDER, fill, 1)\n",
        "                    self.draw('plate', self.tile_size, sl)\n",
        "                    for p in self.env.plate:\n",
        "                        if p.x == x and p.y == y:\n",
        "                            if p.containing:\n",
        "                                self.draw(p.containedName, self.container_size, self.container_location((y, x)))\n",
        "                elif self.env.map[x][y] == ITEMIDX[\"agent\"]:\n",
        "                    for agent in self.env.agent:\n",
        "                        if agent.x == x and agent.y == y:\n",
        "                            self.draw('agent-{}'.format(agent.color), self.tile_size, sl)\n",
        "                            if agent.holding:\n",
        "                                if isinstance(agent.holding, Plate):\n",
        "                                    self.draw('plate', self.holding_size, self.holding_location((y, x)))\n",
        "                                    if agent.holding.containing:\n",
        "                                        self.draw(agent.holding.containedName, self.holding_container_size, self.holding_container_location((y, x)))\n",
        "                                else:\n",
        "                                    self.draw(agent.holding.name, self.holding_size, self.holding_location((y, x)))\n",
        "        pygame.display.flip()\n",
        "        pygame.display.update()\n",
        "\n",
        "        img_int = pygame.PixelArray(self.screen)\n",
        "        img_rgb = np.zeros([img_int.shape[1], img_int.shape[0], 3], dtype=np.uint8)\n",
        "        for i in range(img_int.shape[0]):\n",
        "            for j in range(img_int.shape[1]):\n",
        "                color = pygame.Color(img_int[i][j])\n",
        "                img_rgb[j, i, 0] = color[1]\n",
        "                img_rgb[j, i, 1] = color[2]\n",
        "                img_rgb[j, i, 2] = color[3]\n",
        "\n",
        "        del img_int\n",
        "        return img_rgb\n",
        "\n",
        "    def draw(self, path, size, location):\n",
        "        image_path = '{}/{}.png'.format(graphics_dir, path)\n",
        "        image = pygame.transform.scale(get_image(image_path), size)\n",
        "        self.screen.blit(image, location)\n",
        "\n",
        "    def scaled_location(self, loc):\n",
        "        \"\"\"Return top-left corner of scaled location given coordinates loc, e.g. (3, 4)\"\"\"\n",
        "        return tuple(self.scale * np.asarray(loc))\n",
        "\n",
        "    def holding_location(self, loc):\n",
        "        \"\"\"Return top-left corner of location where agent holding will be drawn (bottom right corner) given coordinates loc, e.g. (3, 4)\"\"\"\n",
        "        scaled_loc = self.scaled_location(loc)\n",
        "        return tuple((np.asarray(scaled_loc) + self.scale*(1-self.holding_scale)).astype(int))\n",
        "\n",
        "    def container_location(self, loc):\n",
        "        \"\"\"Return top-left corner of location where contained (i.e. plated) object will be drawn, given coordinates loc, e.g. (3, 4)\"\"\"\n",
        "        scaled_loc = self.scaled_location(loc)\n",
        "        return tuple((np.asarray(scaled_loc) + self.scale*(1-self.container_scale)/2).astype(int))\n",
        "\n",
        "    def holding_container_location(self, loc):\n",
        "        \"\"\"Return top-left corner of location where contained, held object will be drawn given coordinates loc, e.g. (3, 4)\"\"\"\n",
        "        scaled_loc = self.scaled_location(loc)\n",
        "        factor = (1-self.holding_scale) + (1-self.container_scale)/2*self.holding_scale\n",
        "        return tuple((np.asarray(scaled_loc) + self.scale*factor).astype(int))\n",
        "\n",
        "    def on_cleanup(self):\n",
        "        pygame.display.quit()\n",
        "        pygame.quit()\n",
        "\n",
        "    def get_image_obs(self):\n",
        "        self.screen = pygame.Surface((self.width, self.height))\n",
        "        self.screen.fill(Color.FLOOR)\n",
        "        for x in range(self.env.xlen):\n",
        "            for y in range(self.env.ylen):\n",
        "                sl = self.scaled_location((y, x))\n",
        "                if self.env.map[x][y] == ITEMIDX[\"counter\"]:\n",
        "                    fill = pygame.Rect(sl[0], sl[1], self.scale, self.scale)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER, fill)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER_BORDER, fill, 1)\n",
        "                elif self.env.map[x][y] == ITEMIDX[\"delivery\"]:\n",
        "                    fill = pygame.Rect(sl[0], sl[1], self.scale, self.scale)\n",
        "                    pygame.draw.rect(self.screen, Color.DELIVERY, fill)\n",
        "                    self.draw('delivery', self.tile_size, sl)\n",
        "                    for k in self.env.delivery:\n",
        "                        if k.x == x and k.y == y:\n",
        "                            if k.holding:\n",
        "                                self.draw(k.holding.name, self.tile_size, sl)\n",
        "                                if k.holding.name == \"plate\":\n",
        "                                    if k.holding.containing:\n",
        "                                        self.draw(k.holding.containedName, self.container_size, self.container_location((y, x)))\n",
        "                elif self.env.map[x][y] == ITEMIDX[\"knife\"]:\n",
        "                    fill = pygame.Rect(sl[0], sl[1], self.scale, self.scale)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER, fill)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER_BORDER, fill, 1)\n",
        "                    self.draw('cutboard', self.tile_size, sl)\n",
        "                    for k in self.env.knife:\n",
        "                        if k.x == x and k.y == y:\n",
        "                            if k.holding:\n",
        "                                self.draw(k.holding.name, self.tile_size, sl)\n",
        "                                if k.holding.name == \"plate\":\n",
        "                                    if k.holding.containing:\n",
        "                                        self.draw(k.holding.containedName, self.container_size, self.container_location((y, x)))\n",
        "                elif self.env.map[x][y] == ITEMIDX[\"tomato\"]:\n",
        "                    fill = pygame.Rect(sl[0], sl[1], self.scale, self.scale)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER, fill)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER_BORDER, fill, 1)\n",
        "                    for t in self.env.tomato:\n",
        "                        if t.x == x and t.y == y:\n",
        "                            self.draw(t.name, self.tile_size, sl)\n",
        "                elif self.env.map[x][y] == ITEMIDX[\"lettuce\"]:\n",
        "                    fill = pygame.Rect(sl[0], sl[1], self.scale, self.scale)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER, fill)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER_BORDER, fill, 1)\n",
        "                    for l in self.env.lettuce:\n",
        "                        if l.x == x and l.y == y:\n",
        "                            self.draw(l.name, self.tile_size, sl)\n",
        "                elif self.env.map[x][y] == ITEMIDX[\"onion\"]:\n",
        "                    fill = pygame.Rect(sl[0], sl[1], self.scale, self.scale)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER, fill)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER_BORDER, fill, 1)\n",
        "                    for l in self.env.onion:\n",
        "                        if l.x == x and l.y == y:\n",
        "                            self.draw(l.name, self.tile_size, sl)\n",
        "                elif self.env.map[x][y] == ITEMIDX[\"plate\"]:\n",
        "                    fill = pygame.Rect(sl[0], sl[1], self.scale, self.scale)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER, fill)\n",
        "                    pygame.draw.rect(self.screen, Color.COUNTER_BORDER, fill, 1)\n",
        "                    self.draw('plate', self.tile_size, sl)\n",
        "                    for p in self.env.plate:\n",
        "                        if p.x == x and p.y == y:\n",
        "                            if p.containing:\n",
        "                                self.draw(p.containedName, self.container_size, self.container_location((y, x)))\n",
        "                elif self.env.map[x][y] == ITEMIDX[\"agent\"]:\n",
        "                    for agent in self.env.agent:\n",
        "                        if agent.x == x and agent.y == y:\n",
        "                            self.draw('agent-{}'.format(agent.color), self.tile_size, sl)\n",
        "                            if agent.holding:\n",
        "                                if isinstance(agent.holding, Plate):\n",
        "                                    self.draw('plate', self.holding_size, self.holding_location((y, x)))\n",
        "                                    if agent.holding.containing:\n",
        "                                        self.draw(agent.holding.containedName, self.holding_container_size, self.holding_container_location((y, x)))\n",
        "                                else:\n",
        "                                    self.draw(agent.holding.name, self.holding_size, self.holding_location((y, x)))\n",
        "\n",
        "        img_int = pygame.PixelArray(self.screen)\n",
        "        img_rgb = np.zeros([img_int.shape[1], img_int.shape[0], 3], dtype=np.uint8)\n",
        "        for i in range(img_int.shape[0]):\n",
        "            for j in range(img_int.shape[1]):\n",
        "                color = pygame.Color(img_int[i][j])\n",
        "                img_rgb[j, i, 0] = color[1]\n",
        "                img_rgb[j, i, 1] = color[2]\n",
        "                img_rgb[j, i, 2] = color[3]\n",
        "        # del img_int\n",
        "        return img_rgb"
      ],
      "metadata": {
        "id": "gwv-EcuBZ9Lg",
        "cellView": "form"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Environment\n",
        "DIRECTION = [(0,1), (1,0), (0,-1), (-1,0)]\n",
        "ITEMNAME = [\"space\", \"counter\", \"agent\", \"tomato\", \"lettuce\", \"plate\", \"knife\", \"delivery\", \"onion\"]\n",
        "ITEMIDX= {\"space\": 0, \"counter\": 1, \"agent\": 2, \"tomato\": 3, \"lettuce\": 4, \"plate\": 5, \"knife\": 6, \"delivery\": 7, \"onion\": 8}\n",
        "AGENTCOLOR = [\"blue\", \"magenta\", \"green\", \"yellow\"]\n",
        "TASKLIST = [\"tomato salad\", \"lettuce salad\", \"onion salad\", \"lettuce-tomato salad\", \"onion-tomato salad\", \"lettuce-onion salad\", \"lettuce-onion-tomato salad\"]\n",
        "\n",
        "class Overcooked_V1(gym.Env):\n",
        "\n",
        "    \"\"\"\n",
        "    Overcooked Domain Description\n",
        "    ------------------------------\n",
        "    Agent with primitive actions [\"right\", \"down\", \"left\", \"up\"]\n",
        "    TASKLIST = [\"tomato salad\", \"lettuce salad\", \"onion salad\", \"lettuce-tomato salad\", \"onion-tomato salad\", \"lettuce-onion salad\", \"lettuce-onion-tomato salad\"]\n",
        "\n",
        "    1) Agent is allowed to pick up/put down food/plate on the counter;\n",
        "    2) Agent is allowed to chop food into pieces if the food is on the cutting board counter;\n",
        "    3) Agent is allowed to deliver food to the delivery counter;\n",
        "    4) Only unchopped food is allowed to be chopped;\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\n",
        "        'render.modes': ['human', 'rgb_array'],\n",
        "        'video.frames_per_second' : 5\n",
        "        }\n",
        "\n",
        "    def __init__(self, grid_dim, task, rewardList, map_type = \"A\", n_agent = 2, obs_radius = 2, mode = \"vector\", debug = True):\n",
        "\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        gird_dim : tuple(int, int)\n",
        "            The size of the grid world([7, 7]/[9, 9]).\n",
        "        task : int\n",
        "            The index of the target recipe.\n",
        "        rewardList : dictionary\n",
        "            The list of the reward.\n",
        "            e.g rewardList = {\"subtask finished\": 10, \"correct delivery\": 200, \"wrong delivery\": -5, \"step penalty\": -0.1}\n",
        "        map_type : str\n",
        "            The type of the map(A/B/C).\n",
        "        n_agent: int\n",
        "            The number of the agents.\n",
        "        obs_radius: int\n",
        "            The radius of the agents.\n",
        "        mode: string\n",
        "            The type of the observation(vector/image).\n",
        "        debug : bool\n",
        "            Whehter print the debug information.\n",
        "        \"\"\"\n",
        "        self.action_space = spaces.Discrete(5)\n",
        "        self.observation_space = spaces.Box(-5, 5, dtype=np.float32)\n",
        "\n",
        "\n",
        "        self.xlen, self.ylen = grid_dim\n",
        "        # if debug:\n",
        "        self.game = Game(self)\n",
        "\n",
        "        self.task = task\n",
        "        self.rewardList = rewardList\n",
        "        self.mapType = map_type\n",
        "        self.debug = True\n",
        "        self.n_agent = n_agent\n",
        "        self.mode = mode\n",
        "        self.obs_radius = obs_radius\n",
        "\n",
        "        map = []\n",
        "\n",
        "        if self.xlen == 7 and self.ylen == 7:\n",
        "            if self.n_agent == 2:\n",
        "                if self.mapType == \"A\":\n",
        "                    map =  [[1, 1, 1, 1, 1, 3, 1],\n",
        "                            [6, 0, 2, 0, 2, 0, 4],\n",
        "                            [6, 0, 0, 0, 0, 0, 8],\n",
        "                            [7, 0, 0, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 0, 0, 5],\n",
        "                            [1, 1, 1, 1, 1, 5, 1]]\n",
        "                elif self.mapType == \"B\":\n",
        "                    map =  [[1, 1, 1, 1, 1, 3, 1],\n",
        "                            [6, 0, 2, 1, 2, 0, 4],\n",
        "                            [6, 0, 0, 1, 0, 0, 8],\n",
        "                            [7, 0, 0, 1, 0, 0, 1],\n",
        "                            [1, 0, 0, 1, 0, 0, 1],\n",
        "                            [1, 0, 0, 1, 0, 0, 5],\n",
        "                            [1, 1, 1, 1, 1, 5, 1]]\n",
        "                elif self.mapType == \"C\":\n",
        "                    map =  [[1, 1, 1, 1, 1, 3, 1],\n",
        "                            [6, 0, 2, 1, 2, 0, 4],\n",
        "                            [6, 0, 0, 1, 0, 0, 8],\n",
        "                            [7, 0, 0, 1, 0, 0, 1],\n",
        "                            [1, 0, 0, 1, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 0, 0, 5],\n",
        "                            [1, 1, 1, 1, 1, 5, 1]]\n",
        "            elif self.n_agent == 3:\n",
        "                if self.mapType == \"A\":\n",
        "                    map =  [[1, 1, 1, 1, 1, 3, 1],\n",
        "                            [6, 0, 2, 0, 2, 0, 4],\n",
        "                            [6, 0, 0, 0, 0, 0, 8],\n",
        "                            [7, 0, 0, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 0, 0, 1],\n",
        "                            [1, 0, 2, 0, 0, 0, 5],\n",
        "                            [1, 1, 1, 1, 1, 5, 1]]\n",
        "                elif self.mapType == \"B\":\n",
        "                    map =  [[1, 1, 1, 1, 1, 3, 1],\n",
        "                            [6, 0, 2, 1, 2, 0, 4],\n",
        "                            [6, 0, 0, 1, 0, 0, 8],\n",
        "                            [7, 0, 0, 1, 0, 0, 1],\n",
        "                            [1, 0, 0, 1, 0, 0, 1],\n",
        "                            [1, 0, 2, 1, 0, 0, 5],\n",
        "                            [1, 1, 1, 1, 1, 5, 1]]\n",
        "                elif self.mapType == \"C\":\n",
        "                    map =  [[1, 1, 1, 1, 1, 3, 1],\n",
        "                            [6, 0, 2, 1, 2, 0, 4],\n",
        "                            [6, 0, 0, 1, 0, 0, 8],\n",
        "                            [7, 0, 0, 1, 0, 0, 1],\n",
        "                            [1, 0, 0, 1, 0, 0, 1],\n",
        "                            [1, 0, 2, 0, 0, 0, 5],\n",
        "                            [1, 1, 1, 1, 1, 5, 1]]\n",
        "        elif self.xlen == 9 and self.ylen == 9:\n",
        "            if self.n_agent == 2:\n",
        "                if self.mapType == \"A\":\n",
        "                    map =  [[1, 1, 1, 1, 1, 1, 1, 3, 1],\n",
        "                            [6, 0, 2, 0, 0, 0, 2, 0, 4],\n",
        "                            [6, 0, 0, 0, 0, 0, 0, 0, 8],\n",
        "                            [7, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 0, 0, 0, 0, 5],\n",
        "                            [1, 1, 1, 1, 1, 1, 1, 5, 1]]\n",
        "                elif self.mapType == \"B\":\n",
        "                    map =  [[1, 1, 1, 1, 1, 1, 1, 3, 1],\n",
        "                            [6, 0, 2, 0, 1, 0, 2, 0, 4],\n",
        "                            [6, 0, 0, 0, 1, 0, 0, 0, 8],\n",
        "                            [7, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 1, 0, 0, 0, 5],\n",
        "                            [1, 1, 1, 1, 1, 1, 1, 5, 1]]\n",
        "                elif self.mapType == \"C\":\n",
        "                    map =  [[1, 1, 1, 1, 1, 1, 1, 3, 1],\n",
        "                            [6, 0, 2, 0, 1, 0, 2, 0, 4],\n",
        "                            [6, 0, 0, 0, 1, 0, 0, 0, 8],\n",
        "                            [7, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 0, 0, 0, 0, 5],\n",
        "                            [1, 1, 1, 1, 1, 1, 1, 5, 1]]\n",
        "            elif self.n_agent == 3:\n",
        "                if self.mapType == \"A\":\n",
        "                    map =  [[1, 1, 1, 1, 1, 1, 1, 3, 1],\n",
        "                            [6, 0, 2, 0, 0, 0, 2, 0, 4],\n",
        "                            [6, 0, 0, 0, 0, 0, 0, 0, 8],\n",
        "                            [7, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "                            [1, 0, 2, 0, 0, 0, 0, 0, 5],\n",
        "                            [1, 1, 1, 1, 1, 1, 1, 5, 1]]\n",
        "                elif self.mapType == \"B\":\n",
        "                    map =  [[1, 1, 1, 1, 1, 1, 1, 3, 1],\n",
        "                            [6, 0, 2, 0, 1, 0, 2, 0, 4],\n",
        "                            [6, 0, 0, 0, 1, 0, 0, 0, 8],\n",
        "                            [7, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                            [1, 0, 2, 0, 1, 0, 0, 0, 5],\n",
        "                            [1, 1, 1, 1, 1, 1, 1, 5, 1]]\n",
        "                elif self.mapType == \"C\":\n",
        "                    map =  [[1, 1, 1, 1, 1, 1, 1, 3, 1],\n",
        "                            [6, 0, 2, 0, 1, 0, 2, 0, 4],\n",
        "                            [6, 0, 0, 0, 1, 0, 0, 0, 8],\n",
        "                            [7, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                            [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                            [1, 0, 2, 0, 0, 0, 0, 0, 5],\n",
        "                            [1, 1, 1, 1, 1, 1, 1, 5, 1]]\n",
        "        self.initMap = map\n",
        "        self.map = copy.deepcopy(self.initMap)\n",
        "\n",
        "        self.oneHotTask = []\n",
        "        for t in TASKLIST:\n",
        "            if t == self.task:\n",
        "                self.oneHotTask.append(1)\n",
        "            else:\n",
        "                self.oneHotTask.append(0)\n",
        "\n",
        "        self._createItems()\n",
        "        self.n_agent = len(self.agent)\n",
        "\n",
        "        #action: move(up, down, left, right), stay\n",
        "        self.action_space = spaces.Discrete(5)\n",
        "\n",
        "        #Observation: agent(pos[x,y]) dim = 2\n",
        "        #    knife(pos[x,y]) dim = 2\n",
        "        #    delivery (pos[x,y]) dim = 2\n",
        "        #    plate(pos[x,y]) dim = 2\n",
        "        #    food(pos[x,y]/status) dim = 3\n",
        "\n",
        "        self._initObs()\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(len(self._get_obs()[0]),), dtype=np.float32)\n",
        "        self.obs_space = spaces.Box(low=0, high=1, shape=(len(np.concatenate(self._get_obs(),axis=None)),), dtype=np.float32)\n",
        "\n",
        "\n",
        "    def _createItems(self):\n",
        "        self.agent = []\n",
        "        self.knife = []\n",
        "        self.delivery = []\n",
        "        self.tomato = []\n",
        "        self.lettuce = []\n",
        "        self.onion = []\n",
        "        self.plate = []\n",
        "        self.itemList = []\n",
        "        agent_idx = 0\n",
        "        for x in range(self.xlen):\n",
        "            for y in range(self.ylen):\n",
        "                if self.map[x][y] == ITEMIDX[\"agent\"]:\n",
        "                    self.agent.append(Agent(x, y, color = AGENTCOLOR[agent_idx]))\n",
        "                    agent_idx += 1\n",
        "                elif self.map[x][y] == ITEMIDX[\"knife\"]:\n",
        "                    self.knife.append(Knife(x, y))\n",
        "                elif self.map[x][y] == ITEMIDX[\"delivery\"]:\n",
        "                    self.delivery.append(Delivery(x, y))\n",
        "                elif self.map[x][y] == ITEMIDX[\"tomato\"]:\n",
        "                    self.tomato.append(Tomato(x, y))\n",
        "                elif self.map[x][y] == ITEMIDX[\"lettuce\"]:\n",
        "                    self.lettuce.append(Lettuce(x, y))\n",
        "                elif self.map[x][y] == ITEMIDX[\"onion\"]:\n",
        "                    self.onion.append(Onion(x, y))\n",
        "                elif self.map[x][y] == ITEMIDX[\"plate\"]:\n",
        "                    self.plate.append(Plate(x, y))\n",
        "\n",
        "        self.itemDic = {\"tomato\": self.tomato, \"lettuce\": self.lettuce, \"onion\": self.onion, \"plate\": self.plate, \"knife\": self.knife, \"delivery\": self.delivery, \"agent\": self.agent}\n",
        "        for key in self.itemDic:\n",
        "            self.itemList += self.itemDic[key]\n",
        "\n",
        "\n",
        "    def _initObs(self):\n",
        "        obs = []\n",
        "        for item in self.itemList:\n",
        "            obs.append(item.x / self.xlen)\n",
        "            obs.append(item.y / self.ylen)\n",
        "            if isinstance(item, Food):\n",
        "                obs.append(item.cur_chopped_times / item.required_chopped_times)\n",
        "\n",
        "        obs += self.oneHotTask\n",
        "\n",
        "        for agent in self.agent:\n",
        "            agent.obs = obs\n",
        "        return [np.array(obs)] * self.n_agent\n",
        "\n",
        "\n",
        "    def _get_vector_state(self):\n",
        "        state = []\n",
        "        for item in self.itemList:\n",
        "            x = item.x / self.xlen\n",
        "            y = item.y / self.ylen\n",
        "            state.append(x)\n",
        "            state.append(y)\n",
        "            if isinstance(item, Food):\n",
        "                state.append(item.cur_chopped_times / item.required_chopped_times)\n",
        "\n",
        "        state += self.oneHotTask\n",
        "        return [np.array(state)] * self.n_agent\n",
        "\n",
        "    def _get_image_state(self):\n",
        "        return [self.game.get_image_obs()] * self.n_agent\n",
        "\n",
        "    def _get_obs(self):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        obs : list\n",
        "            observation for each agent.\n",
        "        \"\"\"\n",
        "\n",
        "        vec_obs = self._get_vector_obs()\n",
        "        if self.obs_radius > 0:\n",
        "            if self.mode == \"vector\":\n",
        "                return vec_obs\n",
        "            elif self.mode == \"image\":\n",
        "                return self._get_image_obs()\n",
        "        else:\n",
        "            if self.mode == \"vector\":\n",
        "                return self._get_vector_state()\n",
        "            elif self.mode == \"image\":\n",
        "                return self._get_image_state()\n",
        "\n",
        "    def _get_vector_obs(self):\n",
        "\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        vector_obs : list\n",
        "            vector observation for each agent.\n",
        "        \"\"\"\n",
        "\n",
        "        po_obs = []\n",
        "\n",
        "        for agent in self.agent:\n",
        "            obs = []\n",
        "            idx = 0\n",
        "            if self.xlen == 7 and self.ylen == 7:\n",
        "                if self.mapType == \"A\":\n",
        "                    agent.pomap= [[1, 1, 1, 1, 1, 1, 1],\n",
        "                                  [1, 0, 0, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 0, 0, 1],\n",
        "                                  [1, 1, 1, 1, 1, 1, 1]]\n",
        "                elif self.mapType == \"B\":\n",
        "                    agent.pomap= [[1, 1, 1, 1, 1, 1, 1],\n",
        "                                  [1, 0, 0, 1, 0, 0, 1],\n",
        "                                  [1, 0, 0, 1, 0, 0, 1],\n",
        "                                  [1, 0, 0, 1, 0, 0, 1],\n",
        "                                  [1, 0, 0, 1, 0, 0, 1],\n",
        "                                  [1, 0, 0, 1, 0, 0, 1],\n",
        "                                  [1, 1, 1, 1, 1, 1, 1]]\n",
        "                elif self.mapType == \"C\":\n",
        "                    agent.pomap= [[1, 1, 1, 1, 1, 1, 1],\n",
        "                                  [1, 0, 0, 1, 0, 0, 1],\n",
        "                                  [1, 0, 0, 1, 0, 0, 1],\n",
        "                                  [1, 0, 0, 1, 0, 0, 1],\n",
        "                                  [1, 0, 0, 1, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 0, 0, 1],\n",
        "                                  [1, 1, 1, 1, 1, 1, 1]]\n",
        "            elif self.xlen == 9 and self.ylen == 9:\n",
        "                if self.mapType == \"A\":\n",
        "                    agent.pomap= [[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "                                  [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "                                  [1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
        "                elif self.mapType == \"B\":\n",
        "                    agent.pomap= [[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "                                  [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                                  [1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
        "                elif self.mapType == \"C\":\n",
        "                    agent.pomap= [[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
        "                                  [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 1, 0, 0, 0, 1],\n",
        "                                  [1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
        "                                  [1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
        "\n",
        "            for item in self.itemList:\n",
        "                if item.x >= agent.x - self.obs_radius and item.x <= agent.x + self.obs_radius and item.y >= agent.y - self.obs_radius and item.y <= agent.y + self.obs_radius \\\n",
        "                    or self.obs_radius == 0:\n",
        "                    x = item.x / self.xlen\n",
        "                    y = item.y / self.ylen\n",
        "                    obs.append(x)\n",
        "                    obs.append(y)\n",
        "                    idx += 2\n",
        "                    if isinstance(item, Food):\n",
        "                        obs.append(item.cur_chopped_times / item.required_chopped_times)\n",
        "                        idx += 1\n",
        "                else:\n",
        "                    x = agent.obs[idx] * self.xlen\n",
        "                    y = agent.obs[idx + 1] * self.ylen\n",
        "                    if x >= agent.x - self.obs_radius and x <= agent.x + self.obs_radius and y >= agent.y - self.obs_radius and y <= agent.y + self.obs_radius:\n",
        "                        x = item.initial_x\n",
        "                        y = item.initial_y\n",
        "                    x = x / self.xlen\n",
        "                    y = y / self.ylen\n",
        "\n",
        "                    obs.append(x)\n",
        "                    obs.append(y)\n",
        "                    idx += 2\n",
        "                    if isinstance(item, Food):\n",
        "                        obs.append(agent.obs[idx] / item.required_chopped_times)\n",
        "                        idx += 1\n",
        "\n",
        "                agent.pomap[int(x * self.xlen)][int(y * self.ylen)] = ITEMIDX[item.rawName]\n",
        "            agent.pomap[agent.x][agent.y] = ITEMIDX[\"agent\"]\n",
        "\n",
        "            obs += self.oneHotTask\n",
        "            agent.obs = obs\n",
        "            po_obs.append(np.array(obs))\n",
        "        return po_obs\n",
        "\n",
        "    def _get_image_obs(self):\n",
        "\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        image_obs : list\n",
        "            image observation for each agent.\n",
        "        \"\"\"\n",
        "\n",
        "        po_obs = []\n",
        "        frame = self.game.get_image_obs()\n",
        "        old_image_width, old_image_height, channels = frame.shape\n",
        "        new_image_width = int((old_image_width / self.xlen) * (self.xlen + 2 * (self.obs_radius - 1)))\n",
        "        new_image_height =  int((old_image_height / self.ylen) * (self.ylen + 2 * (self.obs_radius - 1)))\n",
        "        color = (0,0,0)\n",
        "        obs = np.full((new_image_height,new_image_width, channels), color, dtype=np.uint8)\n",
        "\n",
        "        x_center = (new_image_width - old_image_width) // 2\n",
        "        y_center = (new_image_height - old_image_height) // 2\n",
        "\n",
        "        obs[x_center:x_center+old_image_width, y_center:y_center+old_image_height] = frame\n",
        "\n",
        "        for idx, agent in enumerate(self.agent):\n",
        "            agent_obs = self._get_PO_obs(obs, agent.x, agent.y, old_image_width, old_image_height)\n",
        "            po_obs.append(agent_obs)\n",
        "        return po_obs\n",
        "\n",
        "    def _get_PO_obs(self, obs, x, y, ori_width, ori_height):\n",
        "        x1 = (x - 1) * int(ori_width / self.xlen)\n",
        "        x2 = (x + self.obs_radius * 2) * int(ori_width / self.xlen)\n",
        "        y1 = (y - 1) * int(ori_height / self.ylen)\n",
        "        y2 = (y + self.obs_radius * 2) * int(ori_height / self.ylen)\n",
        "        return obs[x1:x2, y1:y2]\n",
        "\n",
        "    def _findItem(self, x, y, itemName):\n",
        "        for item in self.itemDic[itemName]:\n",
        "            if item.x == x and item.y == y:\n",
        "                return item\n",
        "        return None\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return self.get_state().shape[0]\n",
        "\n",
        "    @property\n",
        "    def obs_size(self):\n",
        "        return [self.observation_space.shape[0]] * self.n_agent\n",
        "\n",
        "    @property\n",
        "    def n_action(self):\n",
        "        return [a.n for a in self.action_spaces]\n",
        "\n",
        "    @property\n",
        "    def action_spaces(self):\n",
        "        return [self.action_space] * self.n_agent\n",
        "\n",
        "    def get_avail_actions(self):\n",
        "        return [self.get_avail_agent_actions(i) for i in range(self.n_agent)]\n",
        "\n",
        "    def get_avail_agent_actions(self, nth):\n",
        "        return [1] * self.action_spaces[nth].n\n",
        "\n",
        "    def action_space_sample(self, i):\n",
        "        return np.random.randint(self.action_spaces[i].n)\n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        obs : list\n",
        "            observation for each agent.\n",
        "        \"\"\"\n",
        "\n",
        "        self.map = copy.deepcopy(self.initMap)\n",
        "        self._createItems()\n",
        "        self._initObs()\n",
        "        # if self.debug:\n",
        "        self.game.on_cleanup()\n",
        "\n",
        "        return self._get_obs()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        action: list\n",
        "            action for each agent\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        obs : list\n",
        "            observation for each agent.\n",
        "        rewards : list\n",
        "        terminate : list\n",
        "        info : dictionary\n",
        "        \"\"\"\n",
        "\n",
        "        reward = self.rewardList[\"step penalty\"]\n",
        "        done = False\n",
        "        info = {}\n",
        "        info['cur_mac'] = action\n",
        "        info['mac_done'] = [True] * self.n_agent\n",
        "        info['collision'] = []\n",
        "\n",
        "        all_action_done = False\n",
        "\n",
        "        for agent in self.agent:\n",
        "            agent.moved = False\n",
        "\n",
        "        # if self.debug:\n",
        "        # print(\"in overcooked primitive actions:\", action)\n",
        "\n",
        "        while not all_action_done:\n",
        "            for idx, agent in enumerate(self.agent):\n",
        "\n",
        "                agent_action = action[idx]\n",
        "                if agent.moved:\n",
        "                    continue\n",
        "                agent.moved = True\n",
        "\n",
        "                if agent_action < 4:\n",
        "\n",
        "                    target_x = agent.x + DIRECTION[agent_action][0]\n",
        "                    target_y = agent.y + DIRECTION[agent_action][1]\n",
        "                    target_name = ITEMNAME[self.map[target_x][target_y]]\n",
        "\n",
        "                    if target_name == \"agent\":\n",
        "                        target_agent = self._findItem(target_x, target_y, target_name)\n",
        "                        if not target_agent.moved:\n",
        "                            agent.moved = False\n",
        "                            target_agent_action = action[AGENTCOLOR.index(target_agent.color)]\n",
        "                            if target_agent_action < 4:\n",
        "                                new_target_agent_x = target_agent.x + DIRECTION[target_agent_action][0]\n",
        "                                new_target_agent_y = target_agent.y + DIRECTION[target_agent_action][1]\n",
        "                                if new_target_agent_x == agent.x and new_target_agent_y == agent.y:\n",
        "                                    target_agent.move(new_target_agent_x, new_target_agent_y)\n",
        "                                    agent.move(target_x, target_y)\n",
        "                                    agent.moved = True\n",
        "                                    target_agent.moved = True\n",
        "                    elif  target_name == \"space\":\n",
        "                        self.map[agent.x][agent.y] = ITEMIDX[\"space\"]\n",
        "                        agent.move(target_x, target_y)\n",
        "                        self.map[target_x][target_y] = ITEMIDX[\"agent\"]\n",
        "                    #pickup and chop\n",
        "                    elif not agent.holding:\n",
        "                        if target_name == \"tomato\" or target_name == \"lettuce\" or target_name == \"plate\" or target_name == \"onion\":\n",
        "                            item = self._findItem(target_x, target_y, target_name)\n",
        "                            agent.pickup(item)\n",
        "                            self.map[target_x][target_y] = ITEMIDX[\"counter\"]\n",
        "                        elif target_name == \"knife\":\n",
        "                            knife = self._findItem(target_x, target_y, target_name)\n",
        "                            if isinstance(knife.holding, Plate):\n",
        "                                item = knife.holding\n",
        "                                knife.release()\n",
        "                                agent.pickup(item)\n",
        "                            elif isinstance(knife.holding, Food):\n",
        "                                if knife.holding.chopped:\n",
        "                                    item = knife.holding\n",
        "                                    knife.release()\n",
        "                                    agent.pickup(item)\n",
        "                                else:\n",
        "                                    knife.holding.chop()\n",
        "                                    if knife.holding.chopped:\n",
        "\n",
        "                                        if knife.holding.rawName in self.task:\n",
        "                                            reward += self.rewardList[\"subtask finished\"]\n",
        "                    #put down\n",
        "                    elif agent.holding:\n",
        "                        if target_name == \"counter\":\n",
        "                            if agent.holding.rawName in [\"tomato\", \"lettuce\", \"onion\", \"plate\"]:\n",
        "                                self.map[target_x][target_y] = ITEMIDX[agent.holding.rawName]\n",
        "                            agent.putdown(target_x, target_y)\n",
        "                        elif target_name == \"plate\":\n",
        "                            if isinstance(agent.holding, Food):\n",
        "                                if agent.holding.chopped:\n",
        "                                    plate = self._findItem(target_x, target_y, target_name)\n",
        "                                    item = agent.holding\n",
        "                                    agent.putdown(target_x, target_y)\n",
        "                                    plate.contain(item)\n",
        "\n",
        "                        elif target_name == \"knife\":\n",
        "                            knife = self._findItem(target_x, target_y, target_name)\n",
        "                            if not knife.holding:\n",
        "                                item = agent.holding\n",
        "                                agent.putdown(target_x, target_y)\n",
        "                                knife.hold(item)\n",
        "                            elif isinstance(knife.holding, Food) and isinstance(agent.holding, Plate):\n",
        "                                item = knife.holding\n",
        "                                if item.chopped:\n",
        "                                    knife.release()\n",
        "                                    agent.holding.contain(item)\n",
        "                            elif isinstance(knife.holding, Plate) and isinstance(agent.holding, Food):\n",
        "                                plate_item = knife.holding\n",
        "                                food_item = agent.holding\n",
        "                                if food_item.chopped:\n",
        "                                    knife.release()\n",
        "                                    agent.pickup(plate_item)\n",
        "                                    agent.holding.contain(food_item)\n",
        "                        elif target_name == \"delivery\":\n",
        "                            if isinstance(agent.holding, Plate):\n",
        "                                if agent.holding.containing:\n",
        "                                    dishName = \"\"\n",
        "                                    foodList = [Lettuce, Onion, Tomato]\n",
        "                                    foodInPlate = [-1] * len(foodList)\n",
        "\n",
        "                                    for f in range(len(agent.holding.containing)):\n",
        "                                        for i in range(len(foodList)):\n",
        "                                            if isinstance(agent.holding.containing[f], foodList[i]):\n",
        "                                                foodInPlate[i] = f\n",
        "                                    for i in range(len(foodList)):\n",
        "                                        if foodInPlate[i] > -1:\n",
        "                                            dishName += agent.holding.containing[foodInPlate[i]].rawName + \"-\"\n",
        "                                    dishName = dishName[:-1] + \" salad\"\n",
        "                                    if dishName == self.task:\n",
        "                                        item = agent.holding\n",
        "                                        agent.putdown(target_x, target_y)\n",
        "                                        self.delivery[0].hold(item)\n",
        "                                        reward += self.rewardList[\"correct delivery\"]\n",
        "                                        done = True\n",
        "                                    else:\n",
        "                                        reward += self.rewardList[\"wrong delivery\"]\n",
        "                                        item = agent.holding\n",
        "                                        agent.putdown(target_x, target_y)\n",
        "                                        food = item.containing\n",
        "                                        item.release()\n",
        "                                        item.refresh()\n",
        "                                        self.map[item.x][item.y] = ITEMIDX[item.name]\n",
        "                                        for f in food:\n",
        "                                            f.refresh()\n",
        "                                            self.map[f.x][f.y] = ITEMIDX[f.rawName]\n",
        "                                else:\n",
        "                                    reward += self.rewardList[\"wrong delivery\"]\n",
        "                                    plate = agent.holding\n",
        "                                    agent.putdown(target_x, target_y)\n",
        "                                    plate.refresh()\n",
        "                                    self.map[plate.x][plate.y] = ITEMIDX[plate.name]\n",
        "                            else:\n",
        "                                reward += self.rewardList[\"wrong delivery\"]\n",
        "                                food = agent.holding\n",
        "                                agent.putdown(target_x, target_y)\n",
        "                                food.refresh()\n",
        "                                self.map[food.x][food.y] = ITEMIDX[food.rawName]\n",
        "\n",
        "                        elif target_name in [\"tomato\", \"lettuce\", \"onion\"]:\n",
        "                            item = self._findItem(target_x, target_y, target_name)\n",
        "                            if item.chopped and isinstance(agent.holding, Plate):\n",
        "                                agent.holding.contain(item)\n",
        "                                self.map[target_x][target_y] = ITEMIDX[\"counter\"]\n",
        "\n",
        "            all_action_done = True\n",
        "            for agent in self.agent:\n",
        "                if agent.moved == False:\n",
        "                    all_action_done = False\n",
        "\n",
        "        return self._get_obs(), [reward] * self.n_agent, done, info\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        return self.game.on_render()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UVEUgYakZ4JO",
        "cellView": "form"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initialization for environment\n",
        "\n",
        "env_id='Overcooked-v1'\n",
        "n_agent=2\n",
        "grid_dim=[7, 7]\n",
        "task_id=6\n",
        "map_type='A'\n",
        "obs_radius=2\n",
        "mode='vector'\n",
        "\n",
        "rewardList = {\"subtask finished\": 10, \"correct delivery\": 200, \"wrong delivery\": -5, \"step penalty\": -0.1}\n",
        "TASKLIST = [\"tomato salad\", \"lettuce salad\", \"onion salad\", \"lettuce-tomato salad\", \"onion-tomato salad\", \"lettuce-onion salad\", \"lettuce-onion-tomato salad\"]\n",
        "\n",
        "env=Overcooked_V1(grid_dim, TASKLIST[task_id], rewardList, map_type = \"B\", n_agent = 2, obs_radius = 2, mode = \"vector\", debug = False)\n"
      ],
      "metadata": {
        "id": "p2sJpURbsbbd",
        "cellView": "form"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Printing the outputs\n",
        "def print_out(obs,reward,actionName,step,n_agent,info):\n",
        "  print(\"------------------------------------------------------------------\")\n",
        "  print(\"step: \", step, \"rewards: \", reward)\n",
        "  print(\"#############################################\")\n",
        "\n",
        "  Actiontxts=[\"Blue Agent\",\"Pink Agent\",\"Green Agent\"]\n",
        "  for i in range(n_agent):\n",
        "        print(Actiontxts[i],\" Action: \", actionName[info['cur_mac'][i]])\n",
        "        print(\"Action Done: \", info['mac_done'][i])\n",
        "        print(Actiontxts[i],\" Observation\")\n",
        "        print(\"tomato pos: \", obs[i][0:2]*7)\n",
        "        print(\"tomato status: \", obs[i][2])\n",
        "        print(\"lettuce pos: \", obs[i][3:5]*7)\n",
        "        print(\"lettuce status: \", obs[i][5])\n",
        "        print(\"onion pos: \", obs[i][6:8]*7)\n",
        "        print(\"onion status: \", obs[i][8])\n",
        "        print(\"plate-1 pos: \", obs[i][9:11]*7)\n",
        "        print(\"plate-2 pos: \", obs[i][11:13]*7)\n",
        "        print(\"knife-1 pos: \", obs[i][13:15]*7)\n",
        "        print(\"knife-2 pos: \", obs[i][15:17]*7)\n",
        "        print(\"delivery: \", obs[i][17:19]*7)\n",
        "        print(\"agent-1: \", obs[i][19:21]*7)\n",
        "        print(\"agent-2: \", obs[i][21:23]*7)\n",
        "        print(\"agent-3: \", obs[i][23:25]*7)\n",
        "        print(\"order: \", obs[i][25:])\n",
        "        print(\"#############################################\")\n",
        "        print(\"#############################################\")\n",
        "        print()\n",
        "        print()\n",
        "        print()"
      ],
      "metadata": {
        "id": "yl86oEkCZZs1",
        "cellView": "form"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Play\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import gym\n",
        "from IPython.display import clear_output\n",
        "# from gym_macro_overcooked.macActEnvWrapper import MacEnvWrapper\n",
        "\n",
        "TASKLIST = [\"tomato salad\", \"lettuce salad\", \"onion salad\", \"lettuce-tomato salad\", \"onion-tomato salad\", \"lettuce-onion salad\", \"lettuce-onion-tomato salad\"]\n",
        "\n",
        "def play(env, grid_dim, task, map_type, obs_radius, mode, debug,show_output):\n",
        "    n_agent=env.n_agent\n",
        "\n",
        "    rewardList = {\"subtask finished\": 10, \"correct delivery\": 200, \"wrong delivery\": -5, \"step penalty\": -0.1}\n",
        "    env_params = {'grid_dim': grid_dim,\n",
        "                    'task': TASKLIST[task],\n",
        "                    'rewardList': rewardList,\n",
        "                    'map_type': map_type,\n",
        "                    'n_agent': n_agent,\n",
        "                    'obs_radius': obs_radius,\n",
        "                    'mode': mode,\n",
        "                    'debug': debug\n",
        "                }\n",
        "\n",
        "    #               0        1       2        3     4\n",
        "    actionName = [\"stay\", \"right\", \"down\", \"left\", \"up\"]\n",
        "\n",
        "    rewards = 0\n",
        "    discount = 1\n",
        "    step = 0\n",
        "\n",
        "    obs = env.reset()\n",
        "    # env.render()\n",
        "\n",
        "    action1_data=[]\n",
        "    action2_data=[]\n",
        "    target=[]\n",
        "    step=0\n",
        "    while step<200:\n",
        "\n",
        "\n",
        "        # a = input(\"input:\").split(\" \")\n",
        "        # action = [int(a[0]), int(a[1]), int(a[2])]\n",
        "        action=np.random.randint(5,size=2)\n",
        "        obs, reward, done, info = env.step(action)\n",
        "\n",
        "        action_explanation=\"Agent\"+str(1)+' selects '+actionName[action[0]]\n",
        "        for i in range(1,n_agent):\n",
        "          action_explanation=action_explanation+\" and Agent\"+str(i+1)+' selects '+actionName[action[i]]\n",
        "        action1_data.append(action[0])\n",
        "        action2_data.append(action[1])\n",
        "        target.append(action_explanation)\n",
        "\n",
        "        # print(\"in overcooked primitive actions:\", action_explanation)\n",
        "        # print(\"info\",info)\n",
        "        # print(env.task)\n",
        "\n",
        "\n",
        "        if show_output==1:\n",
        "          clear_output(wait=True)\n",
        "          img_rgb=env.render()\n",
        "          plt.imshow(img_rgb)\n",
        "          plt.pause(0.0001)  # Small pause to update the plot\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        rewards += discount * reward[0]\n",
        "        discount *= 0.99\n",
        "\n",
        "        if show_output==1:\n",
        "          print_out(obs,reward,actionName,step,n_agent,info)\n",
        "        step += 1\n",
        "        if done:\n",
        "            break\n",
        "    return action1_data,action2_data,target\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "grid_dim=[7, 7]\n",
        "task=6\n",
        "map_type='A'\n",
        "obs_radius=2\n",
        "mode='vector'\n",
        "debug=False\n",
        "show_output=0\n",
        "action1_data,action2_data,target=play(env, grid_dim, task, map_type, obs_radius, mode, debug,show_output)\n",
        "df = pd.DataFrame({'action1_data':action1_data,\n",
        "                   'action2_data':action2_data,\n",
        "                   'target':target\n",
        "                   })\n",
        "df.to_csv('experience_data.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "Yvj7MaxUbwpM",
        "cellView": "form"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training\n",
        "def _plot(\n",
        "    agent,\n",
        "    frame_idx: int,\n",
        "    scores: List[float],\n",
        "    losses: List[float],\n",
        "    epsilons: List[float],\n",
        "\n",
        "):\n",
        "\n",
        "    \"\"\"Plot the training progresses.\"\"\"\n",
        "    clear_output(True)\n",
        "    plt.close()\n",
        "    plt.figure(figsize=(16, 4))\n",
        "    # plt.figure()\n",
        "    plt.subplot(131)\n",
        "    # plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))\n",
        "    plt.plot(scores)\n",
        "    plt.xlabel('iteration')\n",
        "    plt.ylabel('Cumulative Reward')\n",
        "    plt.subplot(132)\n",
        "    # plt.figure()\n",
        "    # plt.title('loss')\n",
        "    plt.plot(losses)\n",
        "    plt.xlabel('iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.subplot(133)\n",
        "    # plt.figure()\n",
        "    # plt.title('epsilon')\n",
        "    plt.plot(epsilons)\n",
        "    plt.xlabel('iteration')\n",
        "    plt.ylabel('epsilon')\n",
        "    plt.show()\n",
        "def print_output(agent,print_mode):\n",
        "      if print_mode==1:\n",
        "        for i in range(0,1):\n",
        "          print(\"-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.\")\n",
        "\n",
        "\n",
        "def training(agent1,agent2,agent3, eps_method,max_steps,num_frames, plotting_interval,print_mode):\n",
        "    \"\"\"Train the agent.\"\"\"\n",
        "\n",
        "\n",
        "    state  = agent1.env.reset()\n",
        "    update_cnt = 0\n",
        "    epsilons = []\n",
        "    losses = []\n",
        "    scores = []\n",
        "    score = 0\n",
        "    done=False\n",
        "    rewards=[]\n",
        "    actions=[]\n",
        "    states=[]\n",
        "    for frame_idx in range(0, num_frames + 1):\n",
        "\n",
        "        state  = agent1.reset()\n",
        "\n",
        "\n",
        "\n",
        "        score=[]\n",
        "        stp=0\n",
        "        los=[]\n",
        "\n",
        "        while done==False  and stp<max_steps:\n",
        "\n",
        "          action1 = agent1.select_action(state)\n",
        "          action2 = agent2.select_action(state)\n",
        "          action3 = agent3.select_action(state)\n",
        "          action=[action1,action3]\n",
        "          print(action)\n",
        "\n",
        "\n",
        "          next_state, reward, done,_= agent1.step(action)\n",
        "\n",
        "          rewards.append(reward)\n",
        "          states.append(states)\n",
        "          actions.append(actions)\n",
        "\n",
        "          state = next_state\n",
        "          score.append(reward)\n",
        "\n",
        "\n",
        "          actor_loss, critic_loss = agent3.update_model(next_state)\n",
        "          # actor_losses.append(actor_loss)\n",
        "          # critic_losses.append(critic_loss)\n",
        "          stp+=1\n",
        "\n",
        "\n",
        "        # if training is ready\n",
        "        if len(agent1.memory) >= agent1.batch_size:\n",
        "\n",
        "            loss = agent1.update_model()\n",
        "            los.append(loss)\n",
        "            update_cnt += 1\n",
        "\n",
        "            # linearly decrease epsilon\n",
        "            #Eps 1    : max(min_e,e-(max_e-min_e)*eps_d)\n",
        "            if eps_method==1:\n",
        "\n",
        "              agent1.epsilon = max(\n",
        "                  agent1.min_epsilon, agent1.epsilon - (\n",
        "                      agent1.max_epsilon - agent1.min_epsilon\n",
        "                  ) * agent1.epsilon_decay\n",
        "              )\n",
        "            else:\n",
        "              #Eps2  : max(min_e,e*eps_d)\n",
        "              agent1.epsilon = max(agent1.min_epsilon, agent1.epsilon* agent1.epsilon_decay)\n",
        "\n",
        "            epsilons.append(agent1.epsilon)\n",
        "\n",
        "\n",
        "            # if hard update is needed\n",
        "            if update_cnt % agent1.target_update == 0:\n",
        "                agent1._target_hard_update()\n",
        "        # if frame_idx>0 and frame_idx%35==0:\n",
        "        # agent2.update_policy( rewards, states, actions)\n",
        "        # plotting\n",
        "        if frame_idx % plotting_interval == 0:\n",
        "            _plot(agent1,frame_idx, scores, losses, epsilons)\n",
        "            print_output(agent1,print_mode=1)\n",
        "        scores.append(np.mean(score))\n",
        "        losses.append(np.mean(los))\n",
        "\n",
        "\n",
        "def test(agent):\n",
        "    \"\"\"Train the agent.\"\"\"\n",
        "    agent.is_test = True\n",
        "\n",
        "\n",
        "    update_cnt = 0\n",
        "    epsilons = []\n",
        "    losses = []\n",
        "    scores = []\n",
        "    score = 0\n",
        "    done=0\n",
        "    state= agent.env.reset()\n",
        "    score=0\n",
        "    for step in range(0,max_steps):\n",
        "      agent.env.stp=step\n",
        "      action = agent.select_action(state)\n",
        "      next_state, reward, done,_= agent.step(action)\n",
        "      state = next_state\n",
        "      score += reward\n",
        "    print(\"score: \", score)\n",
        "    print_output(print_mode=1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6MWObsaJPbTJ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "tzol35bM5L41",
        "outputId": "21d37a1c-e08c-45f4-c744-2d2852a5da6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "out_dim 5\n",
            "out_dim 5\n",
            "cpu\n",
            "[2, array([-0.00118555, -0.00416617, -0.00113694,  0.0005507 ,  0.00178603],\n",
            "      dtype=float32)]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-8b3dba630d83>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0mpg_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPolicyGradientAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mppo_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPPOAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrollout_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mentropy_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m   \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpg_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mppo_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_episode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplotting_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprint_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-9bf4c8bdfffc>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(agent1, agent2, agent3, eps_method, max_steps, num_frames, plotting_interval, print_mode)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m           \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0magent1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m           \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-1baee90611ae>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Take an action and return the response of the env.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-8d76ceeec99b>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0magent_action\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m                     \u001b[0mtarget_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mDIRECTION\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_action\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ],
      "source": [
        "# @title Runing Training\n",
        "Train=1\n",
        "if Train==1:\n",
        "  max_steps=200\n",
        "  max_episode=10000\n",
        "\n",
        "  plotting_interval=10\n",
        "  print_mode=0\n",
        "  eps_method=2\n",
        "  state_size = env.obs_space.shape[0]\n",
        "  action_size = env.action_space.n\n",
        "  # parameters\n",
        "\n",
        "  memory_size = 100000 #*****************************************\n",
        "  batch_size = 128  # 128\n",
        "  target_update = 100\n",
        "  epsilon_decay =0.999 #**********************************************************************\n",
        "\n",
        "  dqn_agent = DQNAgent(env, memory_size, batch_size, target_update, epsilon_decay)\n",
        "  pg_agent = PolicyGradientAgent(env)\n",
        "  ppo_agent=PPOAgent(env,gamma = 0.9,tau = 0.8,batch_size = 64,epsilon = 0.2,epoch = 64,rollout_len = 2048,entropy_weight = 0.005)\n",
        "  training(dqn_agent,pg_agent,ppo_agent, eps_method,max_steps,max_episode, plotting_interval,print_mode)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "from collections import deque\n",
        "import random\n",
        "\n",
        "# Set up the environment\n",
        "\n",
        "state_size = env.obs_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "\n",
        "# Initialize the agents\n",
        "ppo_agent = PPOAgent(env, state_size, action_size)\n",
        "pg_agent = PolicyGradientAgent(env, state_size, action_size)\n",
        "dqn_agent = DQNAgent(env, state_size, action_size)\n",
        "\n",
        "# Training hyperparameters\n",
        "num_episodes = 1000\n",
        "max_steps = 200\n",
        "batch_size = 32\n",
        "ppo_update_iters = 10\n",
        "ppo_clip_range = 0.2\n",
        "\n",
        "# Training loop\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "\n",
        "    ppo_states, ppo_actions, ppo_rewards, ppo_dones = [], [], [], []\n",
        "    pg_states, pg_actions, pg_rewards = [], [], []\n",
        "    dqn_states, dqn_actions, dqn_rewards, dqn_next_states, dqn_dones = [], [], [], [], []\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # Select actions and take a step\n",
        "        ppo_action = ppo_agent.select_action(state)\n",
        "        pg_action = pg_agent.select_action(state)\n",
        "        dqn_action = dqn_agent.select_action(state)\n",
        "        next_state, reward, done, _ = env.step(ppo_action)\n",
        "        total_reward += reward\n",
        "\n",
        "        # Store the transitions\n",
        "        ppo_states.append(state)\n",
        "        ppo_actions.append(ppo_action)\n",
        "        ppo_rewards.append(reward)\n",
        "        ppo_dones.append(done)\n",
        "\n",
        "        pg_states.append(state)\n",
        "        pg_actions.append(pg_action)\n",
        "        pg_rewards.append(reward)\n",
        "\n",
        "        dqn_states.append(state)\n",
        "        dqn_actions.append(dqn_action)\n",
        "        dqn_rewards.append(reward)\n",
        "        dqn_next_states.append(next_state)\n",
        "        dqn_dones.append(done)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    print(f'Episode {episode+1}/{num_episodes}, Total Reward: {total_reward}')\n",
        "\n",
        "    # Update the agents\n",
        "    ppo_agent.update(ppo_states, ppo_actions, ppo_rewards, ppo_dones, ppo_update_iters, ppo_clip_range)\n",
        "    pg_agent.update(pg_states, pg_actions, pg_rewards)\n",
        "\n",
        "    if len(dqn_states) >= batch_size:\n",
        "        dqn_agent.update(batch_size)\n",
        "\n",
        "    # Evaluate the agents\n",
        "    ppo_agent_score = evaluate_agent(ppo_agent, env, 10)\n",
        "    pg_agent_score = evaluate_agent(pg_agent, env, 10)\n",
        "    dqn_agent_score = evaluate_agent(dqn_agent, env, 10)\n",
        "    print(f'PPO Agent Score: {ppo_agent_score}, PG Agent Score: {pg_agent_score}, DQN Agent Score: {dqn_agent_score}')\n",
        "\n",
        "def evaluate_agent(agent, env, num_episodes):\n",
        "    total_reward = 0\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            if isinstance(agent, PPOAgent):\n",
        "                action = agent.select_action(state, deterministic=True)\n",
        "            else:\n",
        "                action = agent.select_action(state)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "    return total_reward / num_episodes\n",
        "\n",
        "class PPOAgent:\n",
        "    # PPO agent implementation\n",
        "\n",
        "class PolicyGradientAgent:\n",
        "    # Policy Gradient agent implementation\n",
        "\n",
        "class DQNAgent:\n",
        "    # DQN agent implementation"
      ],
      "metadata": {
        "id": "vQx735vDC2x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuByuwLM_jrT"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGITIgmX-XEB"
      },
      "outputs": [],
      "source": [
        "# Source_of_File=\"PC\"\n",
        "Source_of_File=\"Colab\"\n",
        "if Source_of_File==\"PC\" :\n",
        "  uploaded=files.upload()\n",
        "  filename = next(iter(uploaded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmnPtxr2ez83"
      },
      "outputs": [],
      "source": [
        "\n",
        "Test=1\n",
        "\n",
        "if Test==1 :\n",
        "  if Source_of_File==\"PC\" :\n",
        "    with open('/content/'+filename , \"rb\") as f:\n",
        "      Agent = pickle.load(f)\n",
        "\n",
        "  else:\n",
        "    Agent=agent\n",
        "  Agent.max_it=max_steps\n",
        "  Agent.test()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}